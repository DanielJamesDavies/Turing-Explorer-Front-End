{
	"id": "turing-llm-1.0",
	"title": "Building Turing‑LLM‑1.0‑254M",
	"abstract": "Introducing Turing‑LLM‑1.0‑254M, a novel large language model comprising approximately 254 million parameters, developed specifically for interpretability research purposes. Trained on a dataset of around 2 billion tokens, this model demonstrates a meaningful level of intelligence and general world knowledge. The Turing-LLM framework departs from traditional reliance on web-crawled datasets filled with obscure information, instead adopting a novel approach that utilizes more curated training data. This strategy aims to enhance the interpretability of the model's internal mechanisms, thereby contributing valuable insights to the field of mechanistic interpretability research.",
	"content": [
		[
			{ "type": "text", "style": "heading-1", "text": "Introduction" },
			{
				"type": "text",
				"style": "body",
				"text": "Deep learning has revolutionized AI, with large language models (LLMs) at the forefront, enabling systems to achieve previously unattainable tasks through advanced language understanding, reasoning, and creativity. It has been suggested by some researchers that these models exhibit \"sparks of artificial general intelligence\", generating both excitement for their potential to drive significant advancements, and concern over challenges related to understanding and aligning their behaviours.\n\nUnderstanding the internal mechanisms of LLMs is crucial for aligning these systems with human values and learning from the insights their 'thoughts' provide. However, understanding these models can be challenging due to their complex architecture, vast number of parameters, and the opaque nature of the training process. Mechanistic interpretability aims to understand the inner workings of models, providing insights into how individual components contribute to overall behavior and helping researchers identify and mitigate potential risks.\n\nA major challenge in interpretability is the nature of LLM training data, which often involves large, obscure web-crawled datasets. This ambiguity in training data further complicates efforts to decipher the model's internals. Improving transparency and understanding of training datasets is crucial not only for mitigating risks but also for aligning AI development with human safety and values.\n\nIn this research, we introduce Turing-LLM-1.0-254M, a language model trained on a novel synthetic dataset specifically designed for mechanistic interpretability. By focusing on a smaller-scale model with 254 million parameters, the goal of this research is to make meaningful progress in understanding the internal mechanisms of LLMs. The contributions described represent an incremental yet important step toward a comprehensive understanding of models, enabling safer and more effective integration of AI systems into real-world applications.\n\nIt is my sincere hope that this work may contribute, however modestly, to advancing research that enables us to explore, understand, and learn from digital superintelligence."
			}
		],
		[
			{ "type": "text", "style": "heading-1", "text": "Synthetic Dataset" },
			{
				"type": "text",
				"style": "body",
				"text": "The development of a Large Language Model (LLM) such as Turing-LLM-1.0-254M starts with the creation of a high-quality, high-volume text dataset. The decision to use a synthetic dataset was driven by the hypothesis that understanding the development of a model—including the composition and origin of its training dataset—can yield significant insights into its internal mechanisms. In contrast to traditional datasets, which predominantly consist of web-crawled data and are often too large to be examined in detail, a synthetic dataset offers a controlled and transparent data generation methodology. This transparency enhances the project's understanding of both the data and the model's behaviour\n\nThe use of a synthetic dataset also provides greater assurance regarding ethical standards and content relevance, ensuring alignment with the model's intended objectives and applications. Based on research in model distillation (Ba & Caruana, 2014; Hinton et al., 2015), it was hypothesized that training on synthetic outputs could enhance model performance and interpretability. Thus, the synthetic dataset not only provides the scale necessary for effective training but also a foundation to better explore the capabilities and emergent behaviours of Turing-LLM."
			},
			{ "type": "text", "style": "heading-2", "text": "Generating High-Quality Tokens" },
			{
				"type": "text",
				"style": "body",
				"text": "The first system developed to create the synthetic dataset was the Phi-3 Text Generation System. This system generated the high-quality portion of the dataset. Quality and coherence are key drivers of model performance, so the project needed a sophisticated language model capable of producing high-quality, meaningful text to serve as the dataset's foundation. However, relying on a single model to generate an entire dataset was not computationally feasible. Therefore, Phi-3 was used to establish a high-quality core from which further expansion could be achieved.\n\nPhi-3 Mini (Abdin et al., 2024) was selected to generate the high-quality dataset due to its computational efficiency during inference, superior performance relative to its size, and the open accessibility provided by its MIT license. To develop a diverse and comprehensive dataset, a wide range of subjects was selected, including Physics, Mathematics, Art, Culinary Arts, Communication, and others, to enhance the generalizability of the trained LLM.\n\nThe Phi-3 Text Generation System used a recursive design to ensure in-depth coverage of each subject. It generated a list of topics within a subject, followed by detailed sub-topics under each topic. This hierarchical approach provided depth and consistency across different domains. The system also included user-definable parameters to control the depth of recursive generation, allowing flexibility in dataset creation.\n\nThe Phi-3 Text Generation System was implemented in Python, using the Hugging Face Transformers library for inference with Phi-3-mini-4k-instruct (Abdin et al., 2024). To maximize efficiency, the system used a batch size of 8 and employed bfloat16 precision, reducing computational load and time.\n\nThe system generated different types of content, including subject descriptions, public figures, short stories, and textbooks. The \"subjects\" subset formed the largest portion, with content generated through a structured process of drafting headings and elaborating on them to produce cohesive descriptions. This modular approach allowed the model to generate text that was both contextually rich and coherent."
			},
			{ "type": "text", "style": "heading-2", "text": "Expanding the Dataset" },
			{
				"type": "text",
				"style": "body",
				"text": "To complement the high-quality dataset produced by Phi-3, the project needed an additional system to significantly expand the dataset’s volume. While the Phi-3 Text Generation System provided a high-quality basis, many state-of-the-art LLMs, including Phi-3 Mini, are trained on trillions of tokens (Abdin et al., 2024). The Data Augmentation System was developed to address the challenge of dataset scalability, ensuring a sufficiently large dataset was generated for effective LLM training. \n\n To efficiently generate a larger dataset, the project used a strategy of paraphrasing the high-quality Phi-3 dataset. This approach allowed us to expand the corpus without sacrificing too much quality. Several paraphrasing models were assessed, and \"chatgpt_paraphraser_on_T5_base\" (Vorobev & Kuznetsov, 2023) was selected for its superior paraphrasing quality. This model, a fine-tuned version of \"t5-base\" (Raffel et al., 2020), generated diverse and contextually accurate paraphrases. \n\n The system was designed to process each document generated by Phi-3, split it into manageable chunks, and produce multiple paraphrases for each chunk. This scalable approach expanded the dataset while maintaining semantic richness, helping bridge the gap between high-quality and high-quantity requirements. \n\n The Data Augmentation System was implemented in Python, aiming to maximize efficiency while retaining text quality. Multiple runs were conducted to optimize parameters, and ultimately, each text chunk was paraphrased 11 times for diversity. Techniques like beam search and \"no_repeat_ngram_size\" were used to minimize repetitive output and enhance linguistic variety. Texts were split based on line breaks with a minimum length of 500 characters to ensure coherent paraphrasing. \n\n The augmented dataset comprised batches of paraphrased documents, allowing for the creation of an expansive corpus without compromising the underlying meaning or quality of the content. For each original document, 11 paraphrased versions were generated, providing a large supply of tokens while retaining the core themes of the initial Phi-3 content."
			},
			{ "type": "text", "style": "heading-2", "text": "Final Dataset Formation" },
			{
				"type": "text",
				"style": "body",
				"text": "**Case 1 - Turing-LLM Training Dataset:** The training dataset for Turing-LLM was created by tokenizing all text generated by the Phi-3 and Data Augmentation Systems using the Phi-3 Mini tokenizer. The Phi-3 Text Generation System dataset was split into training and validation subsets, while the augmented dataset was included for the first training epochs. Subsequent epochs focused solely on the high-quality Phi-3 dataset to fine-tune Turing-LLM. \n\n **Case 2 - Interpretability Dataset:** For interpretability research, the dataset was tokenized and split into segments of 64 tokens. These shorter sequences were used as inputs for Turing-LLM during inference, enabling the analysis of latent representations and training of sparse autoencoders. This setup enabled an analysis of how Turing-LLM processed different input features, contributing to ongoing interpretability research."
			}
		],
		[
			{ "type": "text", "style": "heading-1", "text": "Model Architecture" },
			{
				"type": "text",
				"style": "body",
				"text": "To advance the field of interpretability, it was crucial to have a language model that we could deeply investigate. Training Turing-LLM on a synthetic dataset ensured greater predictability of the capabilities of the model, compared to existing models trained on obscure  datasets. Developing a new model for this research provided the opportunity to tune the architecture for optimal efficiency, carefully taking into account the specific hardware constraints of this research. This deliberate design process ensured that the model could achieve peak performance under the limitations of available computational resources."
			},
			{ "type": "text", "style": "heading-2", "text": "Model Architecture Overview" },
			{ "type": "image", "filename": "Architecture.png", "label": "Turing-LLM-1.0-254M Architecture Diagram" },
			{
				"type": "text",
				"style": "body",
				"text": "Turing-LLM-1.0-254M is a large language model designed with a decoder-only transformer architecture. The model consists of 12 block layers, each including Self-Attention and Multi-Layer Perceptron (MLP) components. The size of the model was chosen to strike a balance between computational resource limitations and the potential for developing intelligent behavior. Previous models of comparable scale, such as GPT-2 (Radford et al., 2019), have demonstrated remarkable capabilities, suggesting that a model with 254 million parameters could likewise exhibit interesting emergent features and serve as a fertile ground for interpretability studies.\n\nThe model was built with a block size of 1024 and a vocabulary size of 50,304 tokens. It consists of 12 layers, 16 attention heads, an embedding dimension size of 1024, and an MLP hidden dimension size of 4096. This combination allowed for an expansive and diverse representation of the training data while managing computational feasibility. The vocabulary size represents the number of unique tokens available for use in both inputs and outputs.\n\nTuring-LLM builds upon the foundational work of GPT-2 recreations by Karpathy (2024) and Radford et al. (2019), but diverges in several key aspects. Notable modifications include the use of SwiGLU as the activation function, RMS normalization, the Phi-3 Mini tokenizer, and a unique synthetic dataset. Training was conducted on a single GPU, demonstrating the efficiency of the design choices, even within the constraints of limited hardware resources."
			},
			{ "type": "text", "style": "heading-2", "text": "Detailed Model Specifications" },
			{
				"type": "text",
				"style": "body",
				"text": "For the activation function in the MLP component, SwiGLU (Shazeer, 2020; Ramachandran et al., 2018) was employed. This choice was informed by the activation function's established effectiveness in models like the Llama series (Touvron et al., 2023). It contributes to more efficient representation learning, particularly in terms of balancing compute requirements with model performance.\n\nCurrent state-of-the-art models favor Root Mean Square Layer Normalization (RMS Norm) over traditional layer normalization, as it provides comparable performance with lower computational costs. Following this trend, Turing-LLM adopts RMS normalization to enhance efficiency without sacrificing model output quality.\n\nFor parameter optimization, the AdamW optimizer (Loshchilov & Hutter, 2019; Karpathy, 2024) was utilized. AdamW's adaptive learning rate mechanism made it well suited for handling the variability inherent in training a language model of this scale, ensuring that training was efficient and robust.\n\nA custom learning rate scheduler was used with a max learning rate of 6e-4 and a minimum learning rate of 6e-5. A batch size of 8 was used to maximise compute usage, while gradient accumulation with a total batch size of 524,288 was used to encourage generalisation (Karpathy, 2024) (Radford, et al., 2019)."
			},
			{ "type": "text", "style": "heading-2", "text": "Training Dataset and Tokenization" },
			{
				"type": "text",
				"style": "body",
				"text": "Turing-LLM was trained on the synthetic \"Turing-LLM Training Dataset\", which includes approximately 2 billion tokens. This dataset was carefully curated to ensure comprehensive coverage of linguistic structures relevant to the model's intended applications.\n\nTokenization was performed using the Phi-3 Mini tokenizer (Abdin et al., 2024), selected for its demonstrated effectiveness in generating the high-quality synthetic dataset and its compatibility with the architecture of this model. Employing the same tokenizer used during dataset creation was intended to ensure a more cohesive and precise alignment between input data and model comprehension."
			}
		],
		[
			{ "type": "text", "style": "heading-1", "text": "Model Evaluation" },
			{
				"type": "text",
				"style": "body",
				"text": "After successfully constructing and training Turing-LLM, we now turn our attention to evaluating the model's performance. This evaluation aims to determine whether the model exhibits sufficient intelligence and knowledge to serve as a valuable tool for mechanistic interpretability research."
			},
			{ "type": "text", "style": "heading-2", "text": "Training and Validation Loss" },
			{ "type": "image", "filename": "Loss Curve over Training Steps.png", "label": "Turing-LLM-1.0-254M Loss Curve over Training Steps" },
			{
				"type": "text",
				"style": "body",
				"text": "Figure 2 illustrates that Turing-LLM-1.0-254M achieved a minimum training loss of approximately **1.62** and a validation loss of about **1.80**. For comparison, GPT-2 (124M), a prior leading model of similar scale, as described by Karpathy (2024), achieved a training loss of approximately 3.29. This notable reduction in loss indicates that Turing-LLM-1.0-254M developed a more efficient representation of the training dataset.\n\nThe substantial decrease in loss during the later training stages corresponds with the model being trained exclusively on the synthetic \"Turing-LLM Training Dataset\". This suggests that Turing-LLM effectively learned the high-quality features of this dataset, whereas the initial exposure to a lower-quality dataset likely impeded early progress. These results indicate that Turing-LLM not only captured meaningful representations but also distinguished between different data qualities throughout its training process."
			},
			{ "type": "text", "style": "heading-2", "text": "Relevance for Interpretability" },
			{
				"type": "text",
				"style": "body",
				"text": "Instead of focusing exclusively on traditional benchmarks, the primary goal of this evaluation is to demonstrate that Turing-LLM has developed meaningful intelligence. Such intelligence suggests internal structures within the model that are useful for interpretability research. The presence of these qualities confirms that the model has successfully achieved its objectives."
			},
			{ "type": "text", "style": "heading-3", "text": "Qualitative Analysis" },
			{ "type": "image", "filename": "Turing-LLM-1.0-254M Examples.png", "label": "Turing-LLM-1.0-254M Completion Examples" },
			{
				"type": "text",
				"style": "body",
				"text": "Figure 3 presents selected examples of text completions generated by Turing-LLM when provided with initial prompts consisting of one or two words. These minimal prompts were designed to evaluate the model's ability to generate coherent and contextually appropriate continuations from a limited input. The resulting sequences offer valuable insights into the model's internal knowledge representations and inferential capabilities, showcasing how it processes minimal cues to construct meaningful and relevant text.\n\nAn exploration of Turing-LLM's capabilities revealed an interesting generation when prompted with the single word \"Physics\". The model produced a coherent explanation of atomic orbitals, addressing aspects with appreciable detail. This instance suggests that Turing-LLM retains substantial scientific knowledge and can articulate it clearly, handling complex topics reasonably well.\n\nAnother interesting example came from the simple prompt \"Hello\". Since conversational texts were not a major focus during training, this input could have been challenging for the model. Yet, Turing-LLM managed to generate a suitable response that showed some understanding of conversational context. This suggests that the model can apply what it has learned beyond its core training data, handling unfamiliar inputs reasonably well.\n\nAlthough these examples demonstrate Turing-LLM's ability to intelligently generate texts, it is important to acknowledge that outputs can vary in coherence and truthfulness. Not every generation reaches the same level of accuracy or fluidity, indicating areas where the model could be further improved. Understanding why some generations perform better than others can be valuable, and mechanistic interpretability research can help uncover the specific internal mechanisms responsible for these differences. This insight could guide further refinements, making the model more consistent and reliable."
			},
			{ "type": "text", "style": "heading-3", "text": "Quantitative Analysis" },
			{
				"type": "text",
				"style": "body",
				"text": "The main objective of Turing-LLM-1.0-254M is not to excel in specific tasks but to create a model that can help advance mechanistic interpretability research. To achieve this, internal benchmarks were created to assess how well the model performs on tasks where qualitative analysis suggested it might do particularly well. This helps increase our confidence that Turing-LLM contains meaningful internal structures that are accessible and interpretable.\n\nThe evaluation process followed a simple algorithm:\n1. A \"Writer\" generated 24 topics related to a specific subject.\n2. For each topic, the Writer LLM composed an initial sentence.\n3. Turing-LLM was then prompted with these sentences to generate completions.\n4. An \"Evaluator\" scored the outputs according to a defined benchmark.\n\nThe benchmarks developed centered on topic adherence and language quality. Phi-3 Mini (Abdin et al., 2024), chosen due to its proven intelligence previously demonstrated in this project, was selected to serve as both the Writer and the Evaluator."
			},
			{
				"type": "image",
				"filename": "Average Grade per Subject for Topic Adherence.png",
				"label": "Turing-LLM-1.0-254M Average Grade per Subject for Topic Adherence"
			},
			{
				"type": "image",
				"filename": "Average Grade per Subject for English Quality.png",
				"label": "Turing-LLM-1.0-254M Average Grade per Subject for English Quality"
			},
			{ "type": "image", "filename": "Average Grade per Internal Benchmark.png", "label": "Turing-LLM-1.0-254M Architecture Diagram" },
			{
				"type": "text",
				"style": "body",
				"text": "Results demonstrated that Turing-LLM consistently adhered to topics while also producing high-quality English language text. These observations were further quantified:\n- **Topic Adherence:** Figure 4 reveals that Turing-LLM achieved an average score of 4.93 out of 5 for adherence to the topic, implying a strong ability to both comprehend the initial input and generate thematically consistent text.\n- **English Quality:** As shown in Figure 5, Turing-LLM's average performance received a score of 3.99 out of 5 with minimal variance across subjects. This suggests a reliable level of competence in the grammatical and syntactical quality of its outputs.\n\nThese internal benchmark results strongly imply that Turing-LLM-1.0-254M possesses interesting and interpretable internal features. Its demonstrated ability to produce coherent, relevant, and well-formed content suggests that it is not merely functioning as a regurgitation mechanism but has indeed developed nuanced and interpretable patterns, which aligns with the goals of this research."
			},
			{ "type": "text", "style": "body", "text": "" }
		],
		[
			{ "type": "text", "style": "heading-1", "text": "Conclusion" },
			{ "type": "text", "style": "body", "text": "In-Progress" }
		]
	],
	"references": [
		{
			"id": "karpathy_gpt2_video",
			"title": "Let's reproduce GPT-2 (124M)",
			"author": "Andrej Karpathy",
			"link": "https://www.youtube.com/watch?v=l8pRSuU81PU",
			"year": "2024"
		}
	]
}
